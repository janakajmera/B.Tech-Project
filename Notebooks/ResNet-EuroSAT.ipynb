{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from tqdm import tqdm # For a nice progress bar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. SETUP AND CONFIGURATION\n",
    "# =============================================================================\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2 # 20% of data for validation\n",
    "\n",
    "# Set the device to use for training.\n",
    "# Given your setup, this will automatically select your NVIDIA GTX 1650.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86774a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "# Define transformations for the images.\n",
    "# For transfer learning, we use the normalization stats from the ImageNet dataset,\n",
    "# on which the ResNet model was pre-trained.\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # ResNet models expect 224x224 input\n",
    "    transforms.ToTensor(),         # Convert images to PyTorch Tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet normalization mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet normalization std\n",
    "])\n",
    "\n",
    "# Download and load the EuroSAT dataset\n",
    "# The dataset will be downloaded to a 'data' directory in your project folder.\n",
    "full_dataset = torchvision.datasets.EuroSAT(\n",
    "    root='./Data',\n",
    "    download=True,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "# Get the class names\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Dataset has {len(class_names)} classes: {class_names}\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "num_data = len(full_dataset)\n",
    "num_val = int(VALIDATION_SPLIT * num_data)\n",
    "num_train = num_data - num_val\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [num_train, num_val])\n",
    "print(f\"Number of training images: {len(train_dataset)}\")\n",
    "print(f\"Number of validation images: {len(val_dataset)}\")\n",
    "\n",
    "# Create DataLoaders to handle batching and shuffling\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, # Shuffle training data to improve model generalization\n",
    "    num_workers=2 # Use multiple subprocesses to load data\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a196371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. MODEL DEFINITION (TRANSFER LEARNING)\n",
    "# =============================================================================\n",
    "# Load a pre-trained ResNet18 model.\n",
    "# Using 'ResNet18_Weights.IMAGENET1K_V1' provides the best available weights.\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# The EuroSAT dataset has 10 classes. We need to replace the final layer\n",
    "# of the pre-trained ResNet model, which was originally trained for 1000 ImageNet classes.\n",
    "num_ftrs = model.fc.in_features # Get the number of input features of the final layer\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names)) # Replace it with a new layer for our 10 classes\n",
    "\n",
    "# Move the model to the configured device (GPU or CPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5099fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. LOSS FUNCTION AND OPTIMIZER\n",
    "# =============================================================================\n",
    "# CrossEntropyLoss is standard for multi-class classification.\n",
    "# It combines LogSoftmax and NLLLoss in one single class.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam is a popular and effective optimization algorithm.\n",
    "# We pass the model's parameters and the learning rate.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. TRAINING AND VALIDATION LOOP\n",
    "# =============================================================================\n",
    "# To store metrics for plotting\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Using tqdm for a progress bar\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\")\n",
    "    for inputs, labels in train_pbar:\n",
    "        # Move inputs and labels to the device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 1. Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 3. Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 4. Calculate statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    epoch_train_loss = running_loss / total_samples\n",
    "    epoch_train_acc = correct_predictions / total_samples\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # No need to track gradients during validation, which saves memory and computation\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]\")\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            val_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "\n",
    "    epoch_val_loss = running_loss / total_samples\n",
    "    epoch_val_acc = correct_predictions / total_samples\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "print(\"Finished Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. SAVE THE MODEL\n",
    "# =============================================================================\n",
    "# It's good practice to save the model's state dictionary after training.\n",
    "torch.save(model.state_dict(), 'resnet18_eurosat.pth')\n",
    "print(\"Model saved to resnet18_eurosat.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. VISUALIZE RESULTS\n",
    "# =============================================================================\n",
    "# Plotting accuracy and loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax1.plot(history['train_acc'])\n",
    "ax1.plot(history['val_acc'])\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax2.plot(history['train_loss'])\n",
    "ax2.plot(history['val_loss'])\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
